{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Setup"
      ],
      "metadata": {
        "id": "IGedcVwmH0TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo5oMbj0P-35",
        "outputId": "8823b348-048c-415c-f597-0ed9c2540155"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Yogesh31Hasabe/NCSU-CSC_591-RealTime_AI_and_Machine_Learning_Systems-CourseProject-LPRNet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmY9jdv7MzoB",
        "outputId": "93d9b46e-52ea-4cd4-e6d5-c97e4aed7d75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NCSU-CSC_591-RealTime_AI_and_Machine_Learning_Systems-CourseProject-LPRNet'...\n",
            "remote: Enumerating objects: 1019, done.\u001b[K\n",
            "remote: Counting objects: 100% (1019/1019), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1015/1015), done.\u001b[K\n",
            "remote: Total 1019 (delta 1), reused 1015 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1019/1019), 17.81 MiB | 16.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd NCSU-CSC_591-RealTime_AI_and_Machine_Learning_Systems-CourseProject-LPRNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_YGW4JYKUFM",
        "outputId": "2f159da2-4c6a-4cb7-b5c3-be0f3950622c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NCSU-CSC_591-RealTime_AI_and_Machine_Learning_Systems-CourseProject-LPRNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from model.LPRNet import build_lprnet\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import copy\n",
        "from types import SimpleNamespace\n",
        "from collections import OrderedDict\n",
        "import torch.nn.utils.prune as prune"
      ],
      "metadata": {
        "id": "pNgKbCCwYo1X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'img_size': [94, 24],\n",
        "    'test_img_dirs': \"./data/test\",\n",
        "    'dropout_rate': 0,\n",
        "    'lpr_max_len': 7,\n",
        "    'test_batch_size': 100,\n",
        "    'phase_train': False,\n",
        "    'num_workers': 2,\n",
        "    'cuda': False,\n",
        "    'show': False,\n",
        "    'pretrained_model': './weights/Final_LPRNet_model.pth'\n",
        "}\n",
        "\n",
        "args = SimpleNamespace(**args)"
      ],
      "metadata": {
        "id": "9poWHu9SkTpl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet = build_lprnet(\n",
        "    lpr_max_len=args.lpr_max_len,\n",
        "    phase=args.phase_train,\n",
        "    class_num=len(CHARS),\n",
        "    dropout_rate=args.dropout_rate\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "lprnet.to(device)\n",
        "print(\"Network Built Successfully !! \\n\")\n",
        "print(lprnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4zzCi3DIIGo",
        "outputId": "fcc7468b-dab9-4f8f-ccf6-c15fde3b2314"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Built Successfully !! \n",
            "\n",
            "LPRNet(\n",
            "  (backbone): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): small_basic_block(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
            "        (3): ReLU()\n",
            "        (4): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
            "        (5): ReLU()\n",
            "        (6): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): small_basic_block(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
            "        (3): ReLU()\n",
            "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
            "        (5): ReLU()\n",
            "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): small_basic_block(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
            "        (3): ReLU()\n",
            "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
            "        (5): ReLU()\n",
            "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU()\n",
            "    (14): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): Dropout(p=0, inplace=False)\n",
            "    (16): Conv2d(64, 256, kernel_size=(1, 4), stride=(1, 1))\n",
            "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU()\n",
            "    (19): Dropout(p=0, inplace=False)\n",
            "    (20): Conv2d(256, 68, kernel_size=(13, 1), stride=(1, 1))\n",
            "    (21): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU()\n",
            "  )\n",
            "  (container): Sequential(\n",
            "    (0): Conv2d(516, 68, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained model\n",
        "if args.pretrained_model:\n",
        "    lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
        "    print(\"Pretrained Model loaded successfully !!\")\n",
        "else:\n",
        "    print(\"[Error] Can't found pretrained mode, please check!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKvC4O6JSKJL",
        "outputId": "71137feb-5fd0-44fe-c72d-6cc5e5d6c869"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained Model loaded successfully !!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c91668bc6022>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Accuracy"
      ],
      "metadata": {
        "id": "rW74C_C7ItH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python test_LPRNet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqY6SizVPGe5",
        "outputId": "52dffd79-feb9-4574-a47d-fbd44185c1d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/NCSU-CSC_591-RealTime_AI_and_Machine_Learning_Systems-CourseProject-LPRNet/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.902 [902:57:41:1000]\n",
            "[Info] Test Speed: 0.20261588263511657s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning (Unstructured Prunning)"
      ],
      "metadata": {
        "id": "lJYFwmPxIRmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet_pruned = lprnet\n",
        "module = lprnet_pruned.backbone\n",
        "print(lprnet.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4Oph8mYIUdz",
        "outputId": "12a6df7b-1dbb-4d63-fd6d-0201936daada"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['backbone.0.weight', 'backbone.0.bias', 'backbone.1.weight', 'backbone.1.bias', 'backbone.1.running_mean', 'backbone.1.running_var', 'backbone.1.num_batches_tracked', 'backbone.4.block.0.weight', 'backbone.4.block.0.bias', 'backbone.4.block.2.weight', 'backbone.4.block.2.bias', 'backbone.4.block.4.weight', 'backbone.4.block.4.bias', 'backbone.4.block.6.weight', 'backbone.4.block.6.bias', 'backbone.5.weight', 'backbone.5.bias', 'backbone.5.running_mean', 'backbone.5.running_var', 'backbone.5.num_batches_tracked', 'backbone.8.block.0.weight', 'backbone.8.block.0.bias', 'backbone.8.block.2.weight', 'backbone.8.block.2.bias', 'backbone.8.block.4.weight', 'backbone.8.block.4.bias', 'backbone.8.block.6.weight', 'backbone.8.block.6.bias', 'backbone.9.weight', 'backbone.9.bias', 'backbone.9.running_mean', 'backbone.9.running_var', 'backbone.9.num_batches_tracked', 'backbone.11.block.0.weight', 'backbone.11.block.0.bias', 'backbone.11.block.2.weight', 'backbone.11.block.2.bias', 'backbone.11.block.4.weight', 'backbone.11.block.4.bias', 'backbone.11.block.6.weight', 'backbone.11.block.6.bias', 'backbone.12.weight', 'backbone.12.bias', 'backbone.12.running_mean', 'backbone.12.running_var', 'backbone.12.num_batches_tracked', 'backbone.16.weight', 'backbone.16.bias', 'backbone.17.weight', 'backbone.17.bias', 'backbone.17.running_mean', 'backbone.17.running_var', 'backbone.17.num_batches_tracked', 'backbone.20.weight', 'backbone.20.bias', 'backbone.21.weight', 'backbone.21.bias', 'backbone.21.running_mean', 'backbone.21.running_var', 'backbone.21.num_batches_tracked', 'container.0.weight', 'container.0.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_to_prune = []\n",
        "\n",
        "for name, module in lprnet_pruned.named_modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        parameters_to_prune.append((module, 'weight'))"
      ],
      "metadata": {
        "id": "xt7-WsAlKhAj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_to_prune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJP8ZvZcIjBl",
        "outputId": "01699b3b-44d7-490a-aa61-c9420c79c1a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)), 'weight'),\n",
              " (Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)), 'weight'),\n",
              " (Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)), 'weight'),\n",
              " (Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)), 'weight'),\n",
              " (Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)), 'weight'),\n",
              " (Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)), 'weight'),\n",
              " (Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(64, 256, kernel_size=(1, 4), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(256, 68, kernel_size=(13, 1), stride=(1, 1)), 'weight'),\n",
              " (Conv2d(516, 68, kernel_size=(1, 1), stride=(1, 1)), 'weight')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.9,\n",
        ")\n",
        "\n",
        "for module, _ in parameters_to_prune:\n",
        "    prune.remove(module, 'weight')"
      ],
      "metadata": {
        "id": "r1WTTzxPKvHe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model Weights"
      ],
      "metadata": {
        "id": "MOaBdOo8-Kd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(lprnet_pruned.state_dict(), './weights/lprnet_model_optimization_pruning.pth')"
      ],
      "metadata": {
        "id": "EWFVXCKN-Jqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Function - 1 : Size"
      ],
      "metadata": {
        "id": "cW1K8vXzI-g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Save the model temporarily to measure its size on disk and print the size.\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        model_name (str): Name of the model for reference in output.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
        "    model_size_kb = os.path.getsize(\"temp_delme.p\") / 1e3\n",
        "    print(f\"{model_name} Size (KB): {model_size_kb:.2f}\")\n",
        "    os.remove(\"temp_delme.p\")\n"
      ],
      "metadata": {
        "id": "KVyKxMyqIsCu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size of the model before pruning:')\n",
        "print_size_of_model(lprnet, model_name=\"Original Model\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print('Size of the model after pruning:')\n",
        "print_size_of_model(lprnet_pruned, model_name=\"Pruned Model\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bw3JZnmIuZf",
        "outputId": "9679a48f-8a8c-410e-cddd-58d5d385012e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model before pruning:\n",
            "Original Model Size (KB): 1816.74\n",
            "\n",
            "\n",
            "Size of the model after pruning:\n",
            "Pruned Model Size (KB): 1816.74\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Function - 2 : Accuracy & Speed"
      ],
      "metadata": {
        "id": "0S218oc_Ojhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for sample in batch:\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return torch.stack(imgs, dim=0), labels, lengths\n",
        "\n",
        "\n",
        "def Greedy_Decode_Eval(Net, datasets, args):\n",
        "    # Net.eval()\n",
        "\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets,\n",
        "                                      args.test_batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      num_workers=args.num_workers,\n",
        "                                      collate_fn=collate_fn))\n",
        "\n",
        "    Tp, Tn_1, Tn_2 = 0, 0, 0\n",
        "    t1 = time.time()\n",
        "\n",
        "    for _ in range(epoch_size):\n",
        "\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        targets = []\n",
        "        start = 0\n",
        "\n",
        "        for length in lengths:\n",
        "            targets.append(labels[start:start + length])\n",
        "            start += length\n",
        "        targets = np.array([target.numpy() for target in targets])\n",
        "\n",
        "\n",
        "        if args.cuda:\n",
        "            images = images.cuda()\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prebs = Net(images)\n",
        "\n",
        "\n",
        "        prebs = prebs.cpu().numpy()\n",
        "        preb_labels = []\n",
        "\n",
        "        for preb in prebs:\n",
        "            preb_label = [np.argmax(preb[:, j]) for j in range(preb.shape[1])]\n",
        "            no_repeat_blank_label = []\n",
        "            pre_c = preb_label[0]\n",
        "\n",
        "\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "\n",
        "            for c in preb_label:\n",
        "                if c == len(CHARS) - 1 or c == pre_c:\n",
        "                    pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "\n",
        "\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if np.array_equal(label, targets[i]):\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "\n",
        "\n",
        "    total_samples = Tp + Tn_1 + Tn_2\n",
        "    Acc = Tp / total_samples\n",
        "    t2 = time.time()\n",
        "\n",
        "    print(f\"[Info] Test Accuracy: {Acc:.4f} [Tp:{Tp}, Tn_1:{Tn_1}, Tn_2:{Tn_2}, Total:{total_samples}]\")\n",
        "    print(f\"[Info] Test Speed: {(t2 - t1) / len(datasets):.6f}s per sample [{len(datasets)} samples]\")\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','),\n",
        "                                 args.img_size,\n",
        "                                 args.lpr_max_len)\n",
        "    Greedy_Decode_Eval(model, test_dataset, args)"
      ],
      "metadata": {
        "id": "C6h4781aJCEI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing the model after pruning \\n')\n",
        "test(lprnet_pruned)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcIhKZpuPS0z",
        "outputId": "95bee723-bbdf-4b46-fa07-c33d36724af3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the model after pruning \n",
            "\n",
            "[Info] Test Accuracy: 0.9010 [Tp:901, Tn_1:65, Tn_2:34, Total:1000]\n",
            "[Info] Test Speed: 0.031716s per sample [1000 samples]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}